{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn.preprocessing as pp\n",
    "from queue import PriorityQueue as pq\n",
    "#from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTED_LABELS_FILE \\\n",
    "    = 'predicted_labels2.dat'\n",
    "TRAIN_FILE \\\n",
    "    = 'train.dat'\n",
    "TEST_FILE \\\n",
    "    = 'test.dat'\n",
    "FORMAT_FILE \\\n",
    "    = 'format.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLen(docs, minlen):\n",
    "    r\"\"\" filter out terms that are too short. \n",
    "    docs is a list of lists, each inner list is a document represented as a list of words\n",
    "    minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "    return [ [t for t in d if len(t) >= minlen ] for d in docs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(doc1):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(doc1)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in doc1:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows + 1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in doc1:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(training_set,testing_set):\n",
    "    return training_set * testing_set.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbour_using_knn(l_similarity_matrix_dict_column, l_training_labels, num_of_nearest_neighbors,\n",
    "                            similarity_threshold):\n",
    "    q = pq()\n",
    "    last_key = None\n",
    "    last_value = None\n",
    "    for key,value in l_similarity_matrix_dict_column.items():\n",
    "        last_key = key\n",
    "        last_value = value\n",
    "        if value >= similarity_threshold: # checking for min epsilon\n",
    "            if q.qsize() < num_of_nearest_neighbors:\n",
    "                q.put((value, key))\n",
    "            else:\n",
    "                q.get()\n",
    "                q.put((value, key))\n",
    "    neighbours = []\n",
    "    if not q.empty():\n",
    "        while not q.empty():\n",
    "            training_tuple = q.get()\n",
    "            neighbour = list()\n",
    "            neighbour.append(l_training_labels[training_tuple[1]])\n",
    "            neighbour.append(training_tuple[0])\n",
    "            neighbours.append(neighbour)\n",
    "    else:\n",
    "        neighbour = list()\n",
    "        neighbour.append(l_training_labels[last_key])\n",
    "        neighbour.append(last_value)\n",
    "        neighbours.append(neighbour)\n",
    "    return predict_label(neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(l_neighbours):\n",
    "    labelVotes = {}\n",
    "    labelWeightedVotes = {}\n",
    "    for x in range(len(l_neighbours)):\n",
    "        label = l_neighbours[x][0]\n",
    "        if label in labelVotes:\n",
    "            labelVotes[label] +=1 #majority votes are counted\n",
    "        else:\n",
    "            labelVotes[label] = 1\n",
    "    sorted_votes = sorted(labelVotes, key=labelVotes.__getitem__, reverse=True)    \n",
    "    return sorted_votes[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_measure(l_true_labels, l_predicted_labels):\n",
    "    f1_value = f1_score(l_true_labels, l_predicted_labels, average='weighted')\n",
    "    return f1_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predicted_labels(predicted_labels):\n",
    "    with open(PREDICTED_LABELS_FILE, 'w') as f:\n",
    "        for item in predicted_labels:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    number_of_neighbors = 20\n",
    "    neighbor_filter_threshold = 0.2\n",
    "    # read data from files\n",
    "    with open(TRAIN_FILE, \"r\") as file1:\n",
    "        train_lines = file1.readlines()\n",
    "    with open(TEST_FILE, \"r\") as file2:\n",
    "        test_lines = file2.readlines()\n",
    "    with open(FORMAT_FILE, \"r\") as file3:\n",
    "        format_lines = file3.readlines()\n",
    "\n",
    "    # store data in list of list(str) for all training set, testing set, true labels and a combined list of list(str) of\n",
    "    # training and test set\n",
    "    training_doc = [l.split() for l in train_lines]\n",
    "    training_labels = []\n",
    "    for row in training_doc:\n",
    "        training_labels.append(row[0])\n",
    "        del row[0]\n",
    "    training_doc = filterLen(training_doc,3)\n",
    "    testing_doc = [l.split() for l in test_lines]\n",
    "    testing_doc = filterLen(testing_doc,3)\n",
    "    training_testing_doc = training_doc + testing_doc\n",
    "    true_labels = [l.split() for l in format_lines]\n",
    "\n",
    "    # building matrix for training and test set with terms from both so that columns are same in both matrix\n",
    "    training_testing_mat = build_matrix(training_testing_doc)\n",
    "    training_mat = training_testing_mat[0:len(training_doc)]\n",
    "    testing_mat = training_testing_mat[len(training_doc):len(training_testing_doc)]\n",
    "\n",
    "    # normalizing the training and test data set\n",
    "    training_set = pp.normalize(training_mat.tocsc(), axis=0)\n",
    "    testing_set = pp.normalize(testing_mat.tocsc(), axis=0)\n",
    "\n",
    "    # calculate cosine similarities between training and test data set\n",
    "    similarity_matrix = cosine_sim(training_set, testing_set)\n",
    "\n",
    "    length_test = len(testing_set.todense())\n",
    "\n",
    "    predicted_labels = []\n",
    "    similarity_matrix_dict = {}\n",
    "    similarity_matrix_dense = similarity_matrix.todense()\n",
    "\n",
    "    # convert dense matrix to dict\n",
    "    row_count = np.size(similarity_matrix_dense, 0)\n",
    "    column_count = np.size(similarity_matrix_dense, 1)\n",
    "    for column in range(column_count):\n",
    "        if column % 1000 == 0:\n",
    "            print('column %d' % column)\n",
    "        row_dict = {}\n",
    "        closest_row = 0\n",
    "        closest_similarity = 0.0\n",
    "        for row in range(row_count):\n",
    "            if similarity_matrix_dense[row, column] > neighbor_filter_threshold: #minimum epsilon similarity is checked here\n",
    "                row_dict[row] = similarity_matrix_dense[row, column]\n",
    "            if similarity_matrix_dense[row, column] > closest_similarity:\n",
    "                closest_similarity = similarity_matrix_dense[row, column]\n",
    "                closest_row = row\n",
    "        if len(row_dict) == 0: #if there is no neighbour with similarity > minimum similarity then one nearest neighbor is returned\n",
    "            row_dict[closest_row] = closest_similarity\n",
    "        similarity_matrix_dict[column] = row_dict\n",
    "\n",
    "    for x in range(length_test):\n",
    "        if x % 1000 == 0:\n",
    "            print('Done with index %d ' % x)\n",
    "        n = get_neighbour_using_knn(similarity_matrix_dict[x], training_labels, number_of_neighbors, neighbor_filter_threshold)\n",
    "        predicted_labels.append(n)\n",
    "    # save predicted labels\n",
    "    save_predicted_labels(predicted_labels)\n",
    "    # calculate f-measure score\n",
    "    f1_val = f1_measure(true_labels, predicted_labels)\n",
    "    print(\"f-measure score is %f \" % f1_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
